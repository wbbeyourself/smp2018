{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "BASE = '/home/wb/smp2018'\n",
    "sys.path.append(BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import word2vec\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "from init.config import Config\n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_w2vc(overwrite=True):\n",
    "    if overwrite:\n",
    "        if os.path.exists(cfg.cache_dir + '/w2v_content_word.txt'):\n",
    "            os.remove(cfg.cache_dir + '/w2v_content_word.txt')\n",
    "        if os.path.exists(cfg.cache_dir + '/w2v_content_char.txt'):\n",
    "            os.remove(cfg.cache_dir + '/w2v_content_char.txt')\n",
    "\n",
    "        train_data = get_train_all_data()\n",
    "        vali_data = get_validation_data()\n",
    "\n",
    "        train_content = train_data[\"content\"]\n",
    "        vali_content = vali_data[\"content\"]\n",
    "\n",
    "        print(\"len of train contents\", len(train_content))\n",
    "        print (\"len of vali contents\", len(vali_content))\n",
    "        print(\"len of total contents:\", len(train_content) + len(vali_content))\n",
    "\n",
    "        def applyParallel(contents, func, n_thread):\n",
    "            with Parallel(n_jobs=n_thread) as parallel:\n",
    "                parallel(delayed(func)(c) for c in contents)\n",
    "\n",
    "        def word_content(content):\n",
    "            with open(cfg.cache_dir + \"/w2v_content_word.txt\", \"a+\") as f:\n",
    "                f.writelines(content.lower())\n",
    "                f.writelines('\\n')\n",
    "\n",
    "        def char_content(content):\n",
    "            with open(cfg.cache_dir + \"/w2v_content_char.txt\", \"a+\") as f:\n",
    "                content = content.lower().replace(\" \", \"\")\n",
    "                f.writelines(\" \".join(content))\n",
    "                f.writelines(\"\\n\")\n",
    "\n",
    "        applyParallel(train_content, word_content, 25)\n",
    "        applyParallel(train_content, char_content, 25)\n",
    "        applyParallel(vali_content, word_content, 25)\n",
    "        applyParallel(vali_content, char_content, 25)\n",
    "\n",
    "\n",
    "    # word vector train\n",
    "    model = gensim.models.Word2Vec(\n",
    "        LineSentence(cfg.cache_dir + \"/w2v_content_word.txt\"),\n",
    "        size=vec_dim,\n",
    "        window=5,\n",
    "        min_count=1,\n",
    "        workers=multiprocessing.cpu_count()\n",
    "    )\n",
    "    model.save(cfg.cache_dir + \"/content_w2v_word.model\")\n",
    "\n",
    "    # char vector train\n",
    "    model = gensim.models.Word2Vec(\n",
    "        LineSentence(cfg.cache_dir + '/w2v_content_char.txt'),\n",
    "        size=vec_dim,\n",
    "        window=5,\n",
    "        min_count=1,\n",
    "        workers=multiprocessing.cpu_count()\n",
    "    )\n",
    "    model.save(cfg.cache_dir + \"/content_w2v_char.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- len of train contents 146,341\n",
    "- len of vali contents 58,537\n",
    "- len of toal contents: 204,878"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_w2vc(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_emb(use_opened=False, overwriter=False):\n",
    "\n",
    "    vocab = pickle.load(open(cfg.word_vocab_path, 'rb'))\n",
    "    print(len(vocab))\n",
    "\n",
    "    if use_opened:\n",
    "        word_emb = [np.random.uniform(0, 0, 200) for j in range(len(vocab)+1)]\n",
    "        model = word2vec.load(cfg.open_w2v_path)\n",
    "    else:\n",
    "        word_emb = [np.random.uniform(0, 0, 256) for j in range(len(vocab)+1)]\n",
    "        model = gensim.models.Word2Vec.load(cfg.cache_dir + \"/content_w2v_word.model\")\n",
    "    num = 0\n",
    "    \n",
    "    word_emb = np.array(word_emb)\n",
    "    \n",
    "    for word in vocab:\n",
    "        index = vocab[word]\n",
    "        if word in model:\n",
    "            word_emb[index] = np.array(model[word])\n",
    "            num += 1\n",
    "        else:\n",
    "            word_emb[index] = np.random.uniform(-0.5, 0.5, 256)\n",
    "    print(\"word number: \", num)\n",
    "    print(\"vocab size:\", len(vocab))\n",
    "    print(\"shape of word_emb\", np.shape(word_emb))\n",
    "    if overwriter:\n",
    "        with open(cfg.word_embed_path, 'wb') as f:\n",
    "            pickle.dump(word_emb, f)\n",
    "            print(\"size of embedding_matrix: \", len(word_emb))\n",
    "            print(\"word_embedding finish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- size of word embedding_matrix:  649,130\n",
    "- shape of word_emb (649,130, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_word_emb(False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_char_emb(overwriter=False):\n",
    "\n",
    "    vocab = pickle.load(open(cfg.char_vocab_path, 'rb'))\n",
    "    char_emb = [np.random.uniform(0, 0, 256) for j in range(len(vocab)+1)]\n",
    "    char_emb = np.array(char_emb)\n",
    "    model = gensim.models.Word2Vec.load(cfg.cache_dir + \"/content_w2v_char.model\")\n",
    "    num = 0\n",
    "    for char in vocab:\n",
    "        index = vocab[char]\n",
    "        if char in model:\n",
    "            char_emb[index] = np.array(model[char])\n",
    "            num += 1\n",
    "        else:\n",
    "            char_emb[index] = np.random.uniform(-0.5, 0.5, 256)\n",
    "    print(\"char number: \", num)\n",
    "    print(\"vocab size:\", len(vocab))\n",
    "    print(\"shape of char_emb\", np.shape(char_emb))\n",
    "    if overwriter:\n",
    "        with open(Config.char_embed_path, 'wb') as f:\n",
    "            pickle.dump(char_emb, f)\n",
    "            print(\"size of embedding_matrix: \", len(char_emb))\n",
    "            print(\"char_embedding finish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- size of char embedding_matrix:  7,862\n",
    "- shape of char_emb (7862, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_char_emb(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_vocab(overwriter=False):\n",
    "    word_freq = defaultdict(int)\n",
    "\n",
    "    train_data = get_train_all_data()\n",
    "    vali_data = get_validation_data()\n",
    "    train_content = train_data[\"content\"]\n",
    "    vali_content = vali_data[\"content\"]\n",
    "    \n",
    "    t0 = time.time()\n",
    "\n",
    "    for line in train_content:\n",
    "        line = line.lower().strip()\n",
    "        words = line.split(\" \")\n",
    "        for word in words:\n",
    "            if \" \" == word or \"\" == word:\n",
    "                continue\n",
    "            word_freq[word] += 1\n",
    "    \n",
    "    t1 = time.time()\n",
    "    print('read train data : %s s' % int(t1-t0))\n",
    "\n",
    "    for line in vali_content:\n",
    "        line = line.lower().strip()\n",
    "        words = line.split(\" \")\n",
    "        for word in words:\n",
    "            if \" \" == word or \"\" == word:\n",
    "                continue\n",
    "            word_freq[word] += 1\n",
    "    \n",
    "    t2 = time.time()\n",
    "    print('read vali data : %s s' % int(t2-t0))\n",
    "    \n",
    "    vocab = {}\n",
    "    i = 1\n",
    "    min_freq = 1\n",
    "    for word, freq in word_freq.items():\n",
    "        if freq >= min_freq:\n",
    "            vocab[word] = i\n",
    "            i += 1\n",
    "    vocab['NUM'] = i\n",
    "    vocab['UNK'] = i+1\n",
    "    print(\"size of vocab:\", len(vocab))\n",
    "\n",
    "    if overwriter:\n",
    "        vocab_file = cfg.cache_dir + '/word_vocab.pk'\n",
    "        with open(vocab_file, 'wb') as f:\n",
    "            pickle.dump(vocab, f)\n",
    "        t3 = time.time()\n",
    "        print(\"finish to create vocab; cost : %s s\" % int(t3-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- size of word vocab: 649,129"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_word_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_char_vocab(overwriter=False):\n",
    "    char_freq = defaultdict(int)\n",
    "\n",
    "    train_data = get_train_all_data()\n",
    "    vali_data = get_validation_data()\n",
    "    train_content = train_data[\"content\"]\n",
    "    vali_content = vali_data[\"content\"]\n",
    "    \n",
    "    t0 = time.time()\n",
    "\n",
    "    for line in train_content:\n",
    "        line = line.lower().strip()\n",
    "        line = line.replace(\" \", \"\")\n",
    "        chars_line = \" \".join(line)\n",
    "        chars = chars_line.split(\" \")\n",
    "        for char in chars:\n",
    "            if \" \" == char or \"\" == char:\n",
    "                continue\n",
    "            char_freq[char] += 1\n",
    "    \n",
    "    t1 = time.time()\n",
    "    print('read train data : %s s' % int(t1-t0))\n",
    "\n",
    "    for line in vali_content:\n",
    "        line = line.lower().strip()\n",
    "        line = line.replace(\" \", \"\")\n",
    "        chars_line = \" \".join(line)\n",
    "        chars = chars_line.split(\" \")\n",
    "        for char in chars:\n",
    "            if \" \" == char or \"\" == char:\n",
    "                continue\n",
    "            char_freq[char] += 1\n",
    "    \n",
    "    t2 = time.time()\n",
    "    print('read vali data : %s s' % int(t2-t0))\n",
    "    \n",
    "    vocab = {}\n",
    "    i = 1\n",
    "    min_freq = 1\n",
    "    for char, freq in char_freq.items():\n",
    "        if freq >= min_freq:\n",
    "            vocab[char] = i\n",
    "            i += 1\n",
    "    vocab['NUM'] = i\n",
    "    vocab['UNK'] = i+1\n",
    "    print(vocab)\n",
    "    print(\"size of vocab:\", len(vocab))\n",
    "\n",
    "    if overwriter:\n",
    "        vocab_file = Config.cache_dir + '/char_vocab.pk'\n",
    "        with open(vocab_file, 'wb') as f:\n",
    "            pickle.dump(vocab, f)\n",
    "        t3 = time.time()\n",
    "        print(\"finish to create vocab; cost : %s s\" % int(t3-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- size of char vocab: 7,861"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_char_vocab(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
